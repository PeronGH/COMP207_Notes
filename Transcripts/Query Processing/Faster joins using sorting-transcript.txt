So in this video we'll see how to do joins faster by using sorting. Basically in the last video we saw how to do a naive nested loop join algorithm to handle joins, for instance natural joins. And here we'll see a more advanced algorithm for it called sort join. So the question is, can we do faster than this naive algorithm? And as I've been suggesting until now, yes, we can do faster. Let's see how to do it. To make it clear that I'm not just talking about natural join, I'll here talk about equijoint instead. We haven't seen equijoint before, so let me introduce it. So equijoint of a relation R and S, it uses the same symbol as the natural join symbol, but then has a condition as a subscript. And it's defined as the selection of this condition on R times S. And here A, B are called the join attributes. We'll have that the A comes from R and the B comes from S. So, for instance, if we have these two relations here and we take the equijoint with code equal to depart on these two tables, then we get this as output. So, and this is a key idea, and this we can also use for natural join, because basically equijoints is just a variant of natural joins in some sense, except it let you end up having two columns that are equal in the output instead of a natural join where you move one of them. But still, the principle is the same. If the first table is sorted on A and the second table is sorted on B, then we can compute this equijoint by making one pass over R and one pass over S plus however big the output size is. And that's the running time for it. So let's see how to do that. So let's see how to compute this R equijoint S on A equal to B. And here I'm going to assume that R is already sorted on A, S is sorted on B. If they're not, you can sort them. So we can just assume it's already done. And then we need to do the rest of it. The rest of it is basically merging them two together. It's a similar principle to what you've seen last year about this merge sort, but let me get to the principle here. So we have two tables, R and S, and we want to equijoint them on A equal to B. And R is sorted on A, S is sorted on B. So the output table is over here, and let's try to do it. So we start with the first row in R, the first row in S, and as long as A is smaller than B, we increase A. If the equal of B is bigger, we increase B. So right now A is smaller than B, therefore we go to the next row in R. Still A is smaller than B, so we go to the next row in R. And now we see the two rows are equal. We output the row. So if the two rows are currently equal, as in A is equal to B here, then we're going to increase the row over in S. So we move down here, we put it as part of the output. And then again, because A is equal to B, we move to the next row in S. And we output that also, because they're still equal. And again, we're moving down in S. And now we get to 4, so we go back to A, because A is now smaller than B, and therefore we go into the next row in R. Again, A is smaller than B, so we go into the next row in R. And we go to output this one, and now the two are equal, so we go to the next one in S. And again, the two things are equal. We get this output, and we just continue like this until we're done with the entire file. And that will give you a runtime of this number, as in the size of R plus the size of S plus the size of the output. We have, I think, clearly seen that we can get the size of R. We have clearly seen we can get the size of S. But it's not really clear why I'm also saying that it's part about the output, and I will make it clear over the next few slides. But yeah, this is the runtime for this algorithm. Let me show you an example first before going into formal details about it, where we get this problem with the size of the output, because as we saw before, we didn't really get it, but it happens sometimes. The issue comes up, especially when we have many duplicates in A and B with the same number. As in here, for instance, we have three 2s in A. We have also three 2s in B, and these two go together and will produce nine rows in the output. So the idea of what we're doing here is we'll end up having to remember the tuples in S that match with the current value of A. Let me try to show you. First, we increase the row in R, because, well, one was smaller than two, so we go to the second row here. Now the two things match. We output it, then we increase in B, we output it, we increase in B, we increase in B, and we output it. Now we need to start and output a similar one again, so we start over. First, we look at the first row of S together with the current row of R, and we output those. Then we go to the next row in S, and we output those. We just continue like this over and over again, and keep on outputting rows. What is the runtime of this? Well, it's the size of R plus the size of S plus the size of the output. Why can we see this? Well, it's because in each iteration, we either produce a row of the output, or we move down one row in R, or we move down one row in S, and we never move up unless we have to do this matching stuff, and in that case, we produce more rows of the output. So this is why this is the runtime. So let's look at this faster join algorithm using sorting. So it's called the sort join algorithm. What we do is as follows to compute R, equijoin S on A equal to B, or similarly for network join or some other joins. You sort R on A, you sort S on B, and then you use this approach we saw before to merge the sorted R and S together. You have seen in earlier courses that we can sort in time, well, in log n for n records. Here, the number of records in R is R, or the size of R. So it's R times log base 2 of R. Similarly for S, it's S times log base 2 of S. And finally, we need to do this merging. We have already discussed that the running time is size of R plus the size of S plus the size of the output. And algorithms that are depending on the size of the output are called output sensitive algorithms. And if you think about it, you cannot run faster than the size of the output because, well, at some point in your algorithm, you need to output each of the things that are in the output. Otherwise, where will it come from? So linear in the size of output is good, and everything else is close to linear in the size of the input. So that's in some sense also good. So the total running time is R log base 2 of R plus S log base 2 of S, if you don't have too many values in A that occur multiple times. For instance, this is the case if A is a key. Of course, it's precisely the same thing if we replace A with B. So if the values in B doesn't occur too many times, it again will mean that the output is small, and it will, for instance, be the case if B is a key for this relation of this. However, in general, if you're unlucky, for instance, all the values in A is equal to each other, and it's also equal to all the values in B. Say A only contains two, and B only contains two, then the size of the output is size of R times the size of S. And in that case, it's not better than this naive nested loop join. However, typically, the size of the output is much smaller than this, and therefore we think of this as a better algorithm, typically. Of course, if you know for some reason or another that the output is big, you might not want to use this one. Or it might not really help you much to use this one. But still, it's fairly fast on normal input. So a few remarks. Besides this sort join or this nested loop join, there's a few other algorithms for handling joins, or quite many, actually. There are things like index join, health joins, some multivate joins, where you join multiple tables together, more than just two tables, and each of them are sometimes better than the others. I don't really want to spend too much time on discussing specific way of joining. So I figured sort join was good enough for it. You can also compute the other operations of relational algebra using similar methods to the ones we have seen here. Like I mean, I've already mentioned that you can also use sort join for natural joins and some other joins. And similarly, for many other operations, you can use similar techniques as the ones we have seen here, and the ones we saw in the previous video. So one thing I haven't talked about yet, though, is that the relationships are stored on disk. So why is it important that the relationships are stored on disk? Well, the problem is that our RAM are often not big enough to have all of the database inside them at once. And this is a big problem when you need to sort it, for instance. The problem is that each time you access the disk, you need to spend quite a bit of time to find the right place of it. And then you need to spend quite a bit of time to get the information out from the disk and into the main memory. And these are much, much slower than your RAM. So let me come with some parameters here. Two typical parameters when discussing this kind of thing. We have B, which is the size of disk block. It ranges from roughly 512 to 4096 bytes. So as a half a kilobyte to four kilobytes. And then we have M, which is the number of disk blocks that are handed into available RAM. So, for instance, if you have 16 gigabytes of RAM and your disk block size is 4096, then you can fit four million disk blocks into your RAM. So in that case, M is four million. Or a bit over four million, believe. So if you just use the standard algorithms for reading ablation, as in the ones that are using elementary operations, and it takes you a linear number of operations to read a file consisting of R elements, and it takes you R times log base two of R to sort a file consisting of R records. Of course, you can do it a bit faster using more advanced algorithms, but they're typically not that much faster in practice than this R times log base two of R. However, we can do it a bit faster if we just want to read ablation, if you just measure the number of disk accesses. As I said, disk accesses are much slower than your normal operations. So it makes sense to talk about disk accesses if at least they're close to the number of elementary operations you do. And the number of disk accesses to read a file is R divided by how many things you can store in one disk block, as in this B, and as I said, that's between 512 and 4096. If you assume that each record, each tuple has constant size. And the number of disk accesses, and this is an important part I feel, for sorting can be optimized quite a bit, basically by replacing the two from before by M. I mean, you also replace the R from before by R divided by B, but it's not really so important. The important part is this log base M instead of log base two. I'm going to show you one algorithm that can do this, which is called External Memory Merge Sort. It's probably a bit too advanced to require you to understand it for the exam. So therefore, that video is not going to be required for you to look at for the exam. But still, it will give you an idea of what I mean when I talk about one time versus disk accesses in this video. But again, you will not really be forced to know what an exam is, so if you don't see it, well, it's up to you. So as a summary, in this video, we saw a faster way of doing joins, namely the sort joins. And sort joins basically have time close to linear, well, like normal sorting algorithms, and then plus the size of the output. So if the output is small, the one time is basically linear in the input. If the output is big, well, it's whatever size the output is. And of course, you cannot do better than the size of the output, because your algorithm needs to actually output it at some point. [BLANK_AUDIO]