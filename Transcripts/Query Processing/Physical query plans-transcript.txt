In this video, I will be talking about physical query plans, and it's the last video in this part of the course. So, overview over this video. As I said earlier, well, in this part, we are talking about how to pass queries and how to then optimize them and then finally execute them. And today, I'm going to talk about how to select a physical query plan as an how to decide on exactly what you're going to execute. So, one of these physical query plans, as we'll talk about now, it's meant to add information required to execute your optimized query plan to the query plan. So, we need to select things like what algorithm to use for executing the different operators. Like here, we might want to do the naive selection. Here, we might want to do a selection with an index. If we have an index, we might want to do, say, a sort join or a health join, or perhaps even a nested block join or various other kinds of joins, depending on setup. We might also want to figure out how to pass information from one operator to another. Do we need to write it to disk? Can we just stick it in memory and let it stay there while we move from one step to another? We'll see something called pipelining in this video also, that will also let you do this kind of thing. And we need to figure out for good order for computing joins and unions and stuff like that. You'll see in a bit an example where it matters which order you use for computing the joins. We might also insert additional operations such as sorting. In some cases, it can make things much more efficient and might be better to do it early and stuff like that. So right now, we are given one of these optimized logical query plans as an abelational algebra tree for some query plan. And we can then generate many different physical query plans from this, depending on what choice of algorithm, say, and so on, we make. And then we want to estimate each of the cost of each of these query plans in terms of, say, time, disk accesses, memory, communication and so on. And finally, we should select whichever plan is the cheapest one, it makes sense, and then execute that plan. So how are we going to estimate this cost of execution? Here I'm mainly focusing on the number of disk accesses, as you have seen earlier. That's been quite a focus on it, because as I said, they're typically by far the most expensive part of doing one of these operations. So this number is influenced by many factors. It's especially influenced by the size of the intermediate results, which was what we were trying to optimize earlier, but also how you are passing information around between the different parts of your query, how you are selecting the algorithms for the individual operators and things like that. So how are we doing this estimate then? Well, we are doing it based on parameters of the database. So we need things like the size of the relation, we need some distinct items that are add to boot in each relation, and we can either compute this kind of thing exactly for the database by just going through all of the relation at some point, and then hope not to update the database too much after that, or we can estimate it using statistics. So how are we doing these estimates of the intermediate sizes? Well, it's one of the most challenging tasks in a database management system. It's hard even close to the leaves. Let me come with an example. Say we have this query plan here, and it's already tricky just to figure out what the size is of the selection of the CD of Liverpool on the stores table. This is because you can't afford to execute the query directly. You need to somehow rely on statistics gathered from the data. So if you want a really great estimate on this number, it's really tricky if you don't want to query and we don't have time for it because it will ruin the point. So there are many different approaches, some are easier than others. And if you talk about the size of joints, we enter active resource. Let me try to give you an easy estimate to start up on to give you a flavor of what I'm talking about. So here we're going to estimate the size of the selection. So, for instance, you can estimate the size of A equal to small a, where small a is some constant of r, as the size of r divided by the number of distinct values of this column A obtains in this relation r. And we can also do the estimate on the number of disk blocks similar by, instead of saying the size of r, we just replace it by the number of disk blocks now. And both of these estimates are quite good if each value in A occurs equally often. But it can also be very bad if there is a strong bias towards a specific value. So if it has the specific value, it will be much bigger. If it didn't have that specific value, it would be much smaller than our estimate. So that can be a problem in some cases. Let's use that to combine an estimate of this intermediate result size I mentioned earlier. So say you have a stores relation, it contains 80 tuples and is stored in 20 blocks. And there are exactly four distinct values for the CD attribute of stores. Then the estimate for the size of selection of CD equal to Liverpool in the stores is this 80 divided by the 4. That gives you 20 tuples. So if there are equally many stores in each CD, we will expect Liverpool in this case to have 20 stores. And the estimate for the number of blocks that are required to store this relation is then this number of blocks, 20 divided by the 4 for the number of distinct values, giving you 5 blocks. So let me come with an estimate for join. So here let's talk about neutral join, where we join on the common attribute of A. Then a simple estimate based on the sizes of R and S and a number of distinct values in the common attributes could be something like this. It's size of R times size of S divided by the max number of distinct values for A in R and S. And it's kind of the similar approach to this, what we did in selection. A selection is also based on this assumption that might not always lead to good estimates, as in we are assuming that things are occurring equally often, that things are behaving as they would if we did it uniformly at random. That's not always a good estimate, but it's an approach at least. You can do much more sophisticated methods for this. And it's still an active research area, so you can see papers even in recent conferences in SIGMOD, PODS and VLDB conferences, which is the standard way we're doing for papers in computer science. Besides making these estimates, we also need to think of other things like how are we actually generating these physical query plans? One way would simply be to try all possibilities, as in for each algorithm, try them at each place, see how well it runs, then select the best of them. Typically, you can do it in a more greedy fashion, where you just start from the bottom and move to the top, and whatever has been best so far is probably still going to be best. So let's see if we continue with that, and so on. It gives you a fairly straightforward algorithm for generating query plans. Another thing you might want to look into could be things like what algorithm to use for the different operators. As mentioned earlier, this is also one of the things that can be quite important. And here you need to use the sizes of intermediate results to figure out what to do. I mean, are we going to do a left join or right join on the secret join? Or what are we going to do? And again, whatever choice you make might have big ramifications on how fast you're doing the query. So there's also things like this selection of good join order I mentioned earlier, I believe. And it's also based on the size of the intermediate results. I'll give you an example that can matter what join order you take just to show you that it can matter. I mean, we have seen these other things mattering, at least the suitable algorithm part, and it's quite clear that trying to generate all physical query plans will take you quite a while. So let me try to give you an example of this one. Let me come with one of these examples where the join order can matter for how big the one time is in the end. So here we have select everything from lectures, natural join modules, natural join students, where I am the lecturer. So one way of doing it would be this query tree here. And here we already optimized it a bit by moving the selection down close to the lectures. As I said, this is typically a good way of doing it. So this is a fairly optimized query plan, but you can see that we have chosen to join the students with the modules first and then afterwards join me and afterwards. So if you think about it, say each student has taken 10 modules, then this intermediate result to the left will have size 10 times the number of students. And well, I haven't really done that many modules in total. So when we natural join me onto it, it's going to eliminate most modules so that you'll end up with quite a bit fewer students and modules afterwards. So basically the left intermediate results is gigantic here, while the output is not really that big. Well, it will still be some thousands, I believe. But what if we did it the other way? We took first me, joined it on the modules I've done, and then joined in the students afterwards. Well, the intermediate result, as I said, I haven't done that many modules. Like, this is the third time I'm running this module. So the intermediate result on the right consists of three tuples, and then we are joining it onto the students. So we're getting something like 1200 students in the end, which is clearly much smaller than the number of students that are taking each module, as was our intermediate result before. So again, here we can see that the order we join things in can matter a lot for how big intermediate sizes we get. So now we have seen how to generate the physical query plans. You should probably join it, do it bottom up, say. We have talked a bit about how to select algorithms for the different operations, especially in earlier videos. And we have just seen some examples where the join order matter, are there more issues left? Well, I mentioned it earlier, but there's also the talk about how you pass information from one operation to the next one. There are two general ways of doing it. You can either write the intermediate results out to the hard disk and then read it in again next time you need to use it, or you can use some pipelining, which is a stream-based processing algorithm. So it's basically these two bars, if you're familiar with Linux, where you can sort of combine, first do this, then combine it with this and stuff like that. So basically, what you're doing is without reading things out to the hard disk, you're just passing the tuples from one operation directly to the next one without using the disk. So to do this, we're going to need a bit more buffer space for each pair of adjacent operations to hold the tuples we're currently passing along from one operator to the next one. So let me come up with an example, because it's a bit too abstract to really get from this definition. So the example I'm going to use is where we're going to be doing a projection and a selection. So if you just did the materialization version of it, you would have to do, say, the selection first and then write it out to the hard disk and then read it in again and then do the projection and then write it out. And that will cause you to do twice as much work than what you could do if you just did it once. So what we're doing here is, well, we're first doing the selection. So for each tuple, we check, okay, does this fit with length over 100 and studio equal to Fox, as the selection is requiring. And if it does, instead of writing it out to the hard disk, we're just putting it into the buffer for the next one. And the next one is saying, okay, fine, I should just erase all columns except for the tight linear column. So you don't need to write it out to the disk in this case, for instance, because you can just keep these intermediate results. Let me try using this example to show you how everything fits together, how to select all of these choices. So here we're going to go from a logical query plan, this one, to a physical query plan for the same query. And we have some assumptions. We have our index on city in the stores table and depart is the key for the stores table. Both seems fairly reasonable. So the smartest place to start is probably to say, well, we have only really seen one smart algorithm for joins, for the secret join. It's the sort join and we need to select which one is the outer part and smart to select the left part in this case, because we're going to do it and depart, which we know is the key. And therefore it should be the outer part, as in it should be the one we are not increasing if things are equal. So now we want to do the sort join, we should insert some sorting somewhere. So we could insert it over here to the left and over here to the right and then do the sorting at that point. So it's smart to do it first after having done a selection, because of course it's faster to do sort when you have less data to sort. So we can do a selection down here using our index on the city. It's quite obvious. That's why we have the index on city, that we can do selection faster. And all of these protections, we've only seen one way of doing protection. So that's what we're going to use. There isn't really any other way of doing it, except basically to use all this pipelining. And again, we can insert pipelining pretty much everywhere. There is one important exception here. We cannot do it here just before the sort on either side, because when you do a sort, you have to do multiple passes and that doesn't work with this pipelining. I mean, you can pipelining sort of for the first pass, but you need to do multiples and then the next passes will not be faster based on this pipelining. So, yeah, those two exceptions. And I also want to highlight that this pipeline up here can only be done because the part is the key for stores, because we know that we are not going to go over this right hand side more than once. We're just going to scan down because the left hand side, which is the outer part, the one we are going to keep standing still, if we have equal values in the two columns. And we know that the left part is the key, as in the part is the key for stores. In summary, in this part we have seen how to go from a SQL query to exactly how to solve that SQL query. And more specifically, in this video, we saw how to go from an optimized logical query plan to a physical query plan. And basically select all the algorithms and so on, we are going to use to solve our initial query.