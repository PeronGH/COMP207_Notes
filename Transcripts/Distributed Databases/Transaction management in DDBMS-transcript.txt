This video is about transaction management in distributed database management software. So if you think about it, I feel it's fairly clear that in distributed databases we'll get new problems with transaction management as in all this ACES business we saw earlier in the course. And in this way, in this video, I'm going to discuss some of these issues we can get into and how to deal with the ones I discuss. I will start talking about concurrency control in distributed database management systems. So concurrency control, as you hopefully remember, is a part responsible for ensuring the ACID properties of isolation and consistency. One way of doing that was to use locks. There was other ways, but I'm going to focus on locks here, because it makes for a fairly straightforward discussion about it. So one way of handling locks in distributed databases is to have one computer doing all the locks. So this one computer gets to say, should I grant this request for lock or not? Now this leads to two issues. If this one computer fails, you will need to restart the entire system or the transaction that's running, because you don't know what's locked anymore when this computer fails. So you need to restart everything that's running. And you might also have that two-minute transaction requiring locks at the same time, which will make it hard for this computer, one computer to do all of this. Both are issues, and we can deal with each of them in different ways. Each one has a different way of dealing with it. So one way of dealing with the issue of two-minute transactions is to have more computers, each being the authority for a different item. And the issue about that failure leads to restarts of every transaction, well, one way of dealing with it is to have a backup system running. So let me look at these two dimensions. And as you can see, that simply divides our region into four cases in this straightforward way. So if you have a backup, well, if the primary one fails, we can just keep on using the backup, and things aren't required to restart anymore. That's clearly a good thing. One bad thing about it, though, is that you must keep these two computers synchronized, as in the primary and the backup. Or you can of course have more backups, but again, you still need to keep all the systems synchronized. And that's quite expensive, and that's a bit of a minus for that option. On the other hand, if you add different authorities for different items, well, it has a lot of positive aspects, I feel. But one negative aspect is that it's not clear anymore who you need to ask to get a lock. You need to ask a different person, or a different computer, sorry, depending on which item you are after. And of course, you can also take the combination of both backups and have different authorities for different items. And in that case, you get all the good side and the bad side for this, yeah, from the obvious ones. But again, this is basically an easy, fairly straightforward way of handling locks in distributed databases. So let me mention one more thing before moving on from concurrent to control. Instead of having one computer handling the locks, you can also use voting instead. So the principles are like this. Each site with a copy of an item has a local lock that it can grant transaction for that item. And if a transaction get over half the local locks for an item, it has a global lock on the item. And if so, it must tell all the sites with a copy that it currently has the global lock for the item. If this, if either these two things isn't getting half the locks or telling everybody that you have the lock, if this takes too long, the transaction must stop trying to get the lock and abort this part. So the positive aspect here is that it's much more distributed than a non-voting approach, as in it doesn't matter too much how many goes down. Well, at least as long as half is running, you can still get access to each item. So that's sort of a positive aspect, and it feels much more distributed in some sense. A minus with it though is that it requires much more communication between the different computers to handle this. And that's a big minus, because communication, especially over a network, is quite slow, compared to so many other things in databases. Let me move on from concurrency control to recovery. As you recall, recovery is a part responsible for handling A and D and A set as an atomicity and durability. To be able to discuss what the issue we are dealing with is, I first need to tell you a bit about how to do transactions in these distributed databases. They're not relevant for concurrency control, but they will be for the recovery. So let's revisit our TheaStore chain and see how to do a global transaction here, as in one that uses information from multiple sites. So what you might want to do is, at the central office, you might want to figure out how much inventory is there for some product x at each site, and then move this product between the different stores to balance the inventory. We will not really deal with how to move the things, as in the logistics of it, but only how to update the database. So the global transaction will look something like this. We start a local transaction at the central office, T0 here, and then we instruct the other sites to start local transactions 1, 2, and 3. So we ask them to do something, and they start the transactions 1, 2, and 3. And these smaller transactions 1, 2, and 3, then figure out how much inventory they have on each site, and send this information back to T0. And T0 then determines how to move the product between the different sites, and tells them then how to move the product. And again, as I said, we will not deal with how to move the actual product, just with how to equalize the databases non-botfoid. Now this is the fastest right-forward global transaction. What is the problem here? In this global transaction, we can ensure that each part satisfies atomicity, as in, say, T1 will either be fully executed or not executed at all. But we still have the issue that even if each part has a property, the full thing might not have the property, as in, say, we have executed T1 and T3, but T2 failed. Then, well, the T2 should be rolled back, and therefore the other one should be rolled back as well, because we want the global transactions to either be fully executed or not executed at all. The issue is that nodes can fail while doing this execution of T. So the question there is, should we just abort the global transaction, or is it better to wait, because networks is a bit slow, maybe the just-lord lost the packages, maybe if we wait a bit longer, it will all work out. Another thing we could do is, what if the failed node came back after recovery? Can we continue, or should we abort now, or what should we do? To handle such issues, we use something called distributed commit. So it's a protocol designed to ensure that either everybody commits or no one commits. The protocol for doing distributed commits, or the basic one at least, is called two-phase commit protocol. It's not related whatsoever to the two-phase locking protocol we have seen earlier, except that both have two phases. So let me try to describe it. So the protocol's job is to commit actions globally. So say we have a global transaction starting here in T0, doing some stuff, sending information back and forth, and at some point it wants to commit. And we then have a designated node that decides if and when local transactions can commit. And say it's this one. It could be any other node, but I just picked T0 here. It's a typical choice, but it's not necessarily the one you do. And we then have a bunch of locking. It's done at each node locally, and you also lock the messages that get sent from other nodes, and also the ones you send to other ones. So the two phases, if it follows the first phase, you decide should we abort or should we commit now. And in the second phase you do it. So it's a fairly straightforward protocol. In essence, what happens in phase one is that we need to figure out if we are ready to commit. So the coordinator asks the node if they're ready to commit or not. So this is, say, the coordinator, and it asks by sending our prepare_t methods. So it looks like this. And then after having received these prepare_t methods, each of the nodes decide on their own if right now they're ready to commit. Or if not, they tell the coordinator also that they're not ready. If they're ready, then they go into what we call the pre-committed state and send back the methods ready_t. So it might look like this. Now what is crucial about this pre-committed state is that when you're in this pre-committed state, you're not allowed to abort for any reasons, and it's only the coordinator that's allowed to abort at this point. So if you crash, you must get back to the pre-committed state, and if the system has gone to the committed state at that point, you should also just commit at that point, if you get the information that that TCO choose to commit. On the other hand, if you decide to abort, you can just send back don't commit T and abort the local transaction. So notice in the first case, there is a bit of communication, and we don't actually commit. In the second case, where we abort, we can just abort immediately, because as long as one abort, everybody should abort. So you can delay this, and it might take a while to do this, but at some point you need to decide. So basically you have a timeout, and you figure out that, well, if someone doesn't answer in this time, they probably failed, and you should abort. In phase two, the coordinator then waits for the responses of the nodes, and it assumes that if a node doesn't reply within the time limit, it wishes to abort. So say everybody decided to commit in this case, then they responded with a "d". And in that case, you can ask everybody to commit now. So you send a message commit T, and everybody commits. Now if some node responded don't commit, or didn't respond at all, and it timed out, you send a support T to all nodes, and all nodes abort. As hopefully it's not too surprising, when we talk about recovery, we also need to talk about log files. So what kind of logging do we do here in phase one? Well, first we need to be careful about the order in which we write the log files. So things need to come in the order we want, and we're not allowed to sort of flip them around, because that could lead to issues. It is an issue we discussed earlier, so we are still having this issue here. So in essence, in phase one, we did one of those two things here, and the different sides. First, the coordinator sends out a prepared T, and then each side sends either back a ready T, or don't commit and abort afterwards. And in essence, we just write all of these things in a log file. So before sending a prepared T, you write it in the log file. Before sending a ready T, you write it in the log file. And before sending don't commit T, you write it in the log file. So it's fairly straightforward to remember this part of it. Now, of course, as I said, if you send ready T, you need to enter this pre-committed state, so you also need to ensure this, that T1 doesn't have to be aborted, if you enter this pre-committed state. And at this point, you also need to ensure that all log entries have been written to disk. Finally, if this don't commit T is the last entry in the log, we know we want it to abort, and therefore we can just abort afterwards. So what about phase two? Well, in port terms it's similar. So we have two cases, either all nodes responded with a T to the coordinator, or some node responded don't commit T, or they didn't respond at all. In the former case, we send commit T, in the latter case we send abort T, and we write this stuff in the log file before sending. So you write commit T before sending commit T in the log file in coordinator, and you write abort T in the log file before sending abort T. I believe in the latter case the book says to write don't commit T, I wrote abort here in the slides, I thought it was monotone. So here, if this is, if you see one of these things in log file, you know what the decision was to do, and if there's a failure, you either undo or redo based on it. So if commit T was the last thing in the log file, you know you should commit, and therefore you should redo the transactions, both locally and other places as well. Similarly to commit, if the last thing in the log file was abort T, you know you need to abort everywhere. That was it for the two-phase commit protocol. I will now describe the three-phase commit protocol, which is just fixing a small issue in the two-phase commit protocol. It's really hard to spot if you don't know what you're looking for, so let me try to describe it slowly and see if if that helps. So the issue we're dealing with in the three-phase commit protocol is as follows. If in phase two the coordinator fails and some other transaction fails as well, and everybody else are in the pre-committed state, you cannot do anything. You need to wait until the coordinator or this transaction request are coming back. Why? Well, there are two options to solve this issue. You can either try to abort all the transactions, or you can try to commit all the transactions. So let's see what happens that goes wrong in both cases. So if everybody decides to commit, well, this last transaction request might have decided, "Hey, I need to abort here," or he might have quest too early, but he haven't even gotten this far yet. And in either case, that means that when he comes back online or the coordinator comes back online and tells everybody you need to abort, they roll back something that's committed, and therefore they're back in durability. So that was what happened if everybody decided to commit in this situation here, as in everybody that not quest. What would happen if everybody else, except the things that are quest, except the things that are quest, decided to abort? Well, the issue could then be that the one transaction, or whatever many transactions that had quest, had already been told by the coordinator that you need to commit, because they might already have done so. And again, when they come back online, they will have to roll back, and we are not allowed to roll back committed transactions, and that will again break durability. So in either case, no matter if everybody else choose to abort or commit, they'll break durability. The only option that are allowed under 8-bit would be to stay in this pre-committed state. That means basically keep everything locked that these transactions are using right now of items, and that will be extremely expensive. So you only have a lot of bad choices in this case. So how can we deal with this issue? Well, that's the three-phase commit protocol. The way it does it is as follows. It takes phase two and split it in two. So in the first phase, the first part of phase two, phase 2a, let's call it, we do something we call prepare to commit. So everybody are sent by the coordinator commit or abort. If they're told to abort, they can just immediately abort. It's fine. Like before, there's no issue here. If they're told everybody to commit, they enter a prepare to commit state and send back and tell, hey, I'm ready to commit now. And then after that, the coordinator sends to everybody, you can commit now. Now we can then fail in different states again with similarly an idea. So for instance, if the coordinator fails in 2a now, well, if some of them have heard that you should commit, then everybody can commit. If no one has heard anything, well, they can safely abort because, well, no one can definitely not have been in phase 2b yet because the phase 2a has not been finished. Therefore, it's safe to abort here in phase 2a. On the other hand, if this issue happens in phase 2b, well, we have already been told to commit if we are to commit. So we don't really need phase 2b. We can just commit at this point. It will be fine because we know that we are supposed to commit in the future. So yeah, so we can deal with this problem here. So as a summary of this video, we saw a few ways to deal with issues to do with concurrency, specifically how to deal with locks in distributed database management system. We had two dimensions. We could have different computers for dealing with different database items or not. We just have one computer for dealing with all database items and computers could have backup or not. And again, there's disadvantages with each of these options and you can select which one you like. We also saw voting as an alternative version of it. It was a bit more expensive, but somewhat more distributed. Finally, for recovery, we have seen the two-phase commit protocol and the three-phase commit protocol. And again, I want to highlight here that it has no relation whatsoever to the two-phase locking besides the words two and phase. Well, there are two or three phases, but they have basically nothing to do with the two-phase commit protocol. I did not deal with deadlocks in this video. Again, you can run into deadlocks in new ways in distributed databases. I didn't really want to talk about it.