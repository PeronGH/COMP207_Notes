This part is about being sure to be able to recover no matter what happens in your schedules. So far back in time, back in the video on good schedules and transactions in this part, it was one of the first videos in here in part 2. We saw a problem called problem 3 that didn't have any good ways you could recover from it. And we would like to do something about it. So far in this part 2, we have seen two things. We have seen conflict serializability and we have seen recovery. Conflict serializability has many nice properties, like they're basically what's the same as a serial schedule, as in has the same good properties of serial schedules while still allowing some amount of concurrency. So basically, they ensured consistency and they ensured correctness of the schedule. And we could do them using these two-phase logging. On the other hand, we have seen, or just seen, logging and recovery and we have seen how to restore a database back to desired states, like when error of failure occurs or something like that. So we can undo incomplete transactions, we can redo completed transactions, or if we are bored of transactions, we also know how to do that. We just use this undo logging again. And it works even if we have a power failure or other kinds of failure. However, right now, these two sides doesn't work that well together. The problem is this problem 3 I showed in the beginning. So formally speaking, the problem is we have something called cascading rollbacks. And right now, under these two techniques we have seen, they might be necessary. So let me go back to this problem 3 I mentioned I wanted to talk about. It was the one that looked like this. We had two users, user 1 and user 2. User 1 inserted 100 pounds into account 456 and then had some data point to try to insert money from another, the same 100 pounds from another account. And then user 2 came along, checked there was enough money on account 456, added some money into another account, and subtracted some money for 456. And we could do it using these commands here. I will not go through it all. I mean, if you want to see all the details, go back and watch this good schedules video where we went through it. But right now, the issue comes if you have a failure here between the end of user 2 doing his thing and user 1 removing the money from 1 to 3. So the problem would be that this last command, this remove money from 1 to 3, would not happen in that case. And this is going to lead us into a lot of problems, especially if user 2 has already committed at the point of the failure. So I want to highlight here that this schedule here has a reasonable number of good properties. For instance, it could be done using 2PL, for instance, if user 1 locked both 456 and 123 initially, unlocked 456 just after having added the money in, and user 2 locked his two accounts he's interested in and unlocked them after he'd finished. Then that would be 2PL because both all the locking was done before all the unlocking, and that was all that was required from 2PL. So especially this is conflict-sablizable. So what did we do about it? Well, we looked at some options for what we could do, and we found out that all of them was bad for one reason or another. Let's look at them again. So one thing we could do, we could do absolutely nothing about this, but then the bank ended up losing money because the money that should go out of 123 was not done. And besides that, it would break atomicity because part of what user 1 would like to do in his transaction was to add money into one account and remove it from another account. If only the first half was done, that would break atomicity because it requires either everything to be done or nothing to be done. Another option was that we could undo the first-hand section but not the second one, but then we might end up breaking some integrity constraints. Like, we might have a constraint saying that you should have a positive amount of money on your bank account, and if you undo the first one, it has lost 100 pounds compared to what it should have. And that was sort of what this check, there was not enough money on the bank account for supposed to be talking about. Now, that would break consistency because we'll break this integrity constraint, and we'll also be breaking isolation as well. Well, at least some levels of it. I mean, we are doing read uncommitted here. That's a bit of a problem. And it will also be inconsistent. I don't mean here like inconsistency, but what a bot should be doing is it should remove everything a transaction has done. And if other transaction uses it already, well, it should remove them too because it should be like this transaction never happened. And right now, part of this transaction, part of the effect of this transaction seemingly happened because transaction two acted on it. So the last option what we could do was undo both transactions, but you two might already have gone away and committed and everything, and therefore we'll end up breaking into ability because you are not allowed to undo committed transactions. So in many times in real life, we could in some sense run into this problem that this isolation property is not fully enforced, so we could get this dirty reads. And the reason why we might do so in real life is that it gives us more efficiency. We don't need to spend time on doing all this shared logs and stuff like that, and we can gain more parallelism if we don't have to let things wait until they can get their shared logs. And as we have seen earlier, you can decide on your own how much isolation you want, and for instance, if you're in this command, you would be able to do this dirty reads because the problem here is that we can read something that's not committed. There's another option up here. You can also say read only instead of read write. That means that you can only read, you're not allowed to do any writes. And instead of this isolation level read uncommitted, you can also do read committed, which will already prevent this dirty reads or repeatable read or serializable, and both of this will also prevent dirty reads. So the problem with dirty reads, though, is that when you have to abort, it will take longer because you might have to abort multiple transactions in this cascading fashion, as we saw earlier. Let me come back an example where we can see it. So what ended up happening in this kind of case is we end up having something called cascading rule-backs. So say we have a transaction T1 here that aborts. Well, we need to find everything that read something written by T1. Say that might be T2 and T3. And now they should also abort because abort should be as if transaction one never happened, and therefore T2 and T3 could not read from it. The only way to ensure that is to abort T2 and T3. And you need to recursively abort transactions here. Everything that has read something written by these aborted transactions. So for T2, it might have been read by T4 and T5, and so on. So we just have to abort T4 and T5, and we might build a big tree like this, and we have to abort everything in this big tree. And well, this issue up here, it's very slow and we want to avoid this. And as I said, this issue is called cascading rule-backs. We had two options for doing it. Well, we could try not to abort the support, but then we'll break isolation. And more importantly, it will be inconsistent with what abort should be doing. That is, ensure that this transaction you aborted never happened in the first place. And on the other hand, if we do break them, then we break durability. Let me come with an explicit example where we have this choice between isolation or inconsistency abort and durability. So the example is as follows. We have two transactions here, transaction one and transaction two. Transaction one reads X at 100 to it, and a long while later ends up aborting at time 12. At the same time, we have transaction two coming in, reading this X that was done by transaction one, multiplying it by two and writing it out and committing. Especially, it commits before transaction one aborts. And now when we abort transaction one, we can either choose not to abort transaction two, that would break isolation, and it wouldn't ensure that everything done by transaction one was as if it didn't happen in the first place. On the other hand, we could abort transaction two, and then we'll be breaking durability, because transaction two has already committed. So both things are bad, we want to avoid this. So let me talk about it a bit more abstractly. What is the problem for durability? Well, it's situations like this, where we have a transaction, say T2, it reads something from another transaction, say T1, then T2 commits, and afterwards T1 aborts, for some reason or another. So we're saying a schedule is recoverable, as we can ensure durability in it. If the following is true, if a transaction T1 commits and has written an item that was written by another transaction, then the other transaction must commit before T1 does. So this very precisely removes this issue. So the schedules must look like this. If you have T2 writes to X, and T1 reads X afterwards, then we must have that if T1 commits at some point, then before that, T2 must have already committed. So between T1 reading X and T1 committing, T2 must have committed already, to be able to have this recoverable property that's necessary to ensure durability. So if you look at it carefully, we can still do cascading rollbacks, but only with active transactions that are not already committed, and therefore will not be breaking durability. So let's look at some examples of this. The first example I will mention is this one, S1. Here we have that T2 reads data written by transaction 1, and transaction 1 commits before transaction 2, and therefore this is a recoverable schedule, because there are no other reads here. On the other hand, this is a non-recoverable schedule. Again, transaction 2 reads data written by transaction 1, but transaction 2 commits before transaction 1. So notice that if you have flipped these two last commit statements, then we will be back to our recoverable schedule, and everything will be fine, but because we commit C2 before C1, there is an issue here. Now, if you look carefully, S1 is in general not serializable. This is because if you look carefully, you will see that transaction 1 ends up writing to X in S1, and transaction 2 ends up writing to Y in S1. And this will not happen in a serial schedule, because both transactions write to both variables, both X and Y. So that's sort of a problem, and this is why S1 is not serializable. On the other hand, S2 is serializable. It's actually basically serial, except that C commits transaction 1 happens quite late. So if commit 1, and normally when we talk about serializability and conflict serializability and so on, we don't care about commit statements, so that's why this S2 here is serializable, because it's basically serial. For this to make sense, we need an implicit assumption on our log files, namely that things you write to the log file must end up on disk in the order written. So this schedule here, S1, is recoverable, and if you just flip the commit statements, then it's not recoverable. But if S1, if you have actually written, done S1, and then just by mistake had flipped in a log file the two commit statements, then in principle at least we could have done a cascading rollback here with a committed transaction, and that would be bad. So therefore you need to ensure that the log file contains the commit statement in the right order. So in this video we are starting to discuss how we can get conflict serializability to work with recovery. So if we just do it naively, we end up having cascading rollbacks, and this leads into a problem with either durability or isolation, and besides isolation you would also want to run into this issue that aborts should be as if the transaction you aborted never happened. Therefore we should try to avoid them, and at least to avoid the part with durability we can use as recoverable schedules. So the recoverable schedules ensures that you can recover from any situation. And that's what we want. So recoverable schedules are defined as follows. A transaction commits only if all transactions it has read from has already committed. And that's the requirement to have a recoverable schedule.