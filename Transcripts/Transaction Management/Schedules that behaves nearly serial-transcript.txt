So in this video I will discuss schedules that behaves nearly serial. As we saw earlier in the video on ACID or on this intuitive talk about ACID, we saw that if things behave nearly serial, then we'll satisfy both isolation and consistency. And as I said earlier, this is what we want to do in this next part. How can we satisfy isolation and consistency alone? So basically, how much concurrency can be allowed while still being close enough to a serial schedule that we still have all the properties we want as consistency and isolation. So we have seen two kinds of schedules by now. We have seen serial schedules. They are quite simple. They want to sort of do one transaction first, then the next transaction, then the next transaction, and so on. And they ensure consistency of the database, because, well, we have that our assumption saying that if we have started in a consistent state, do a transaction entirely on its own, then we end up in a consistent state. So first we do the first transaction on its own, then that means we end up in a consistent state because of this assumption. Then we do the next transaction on its own, again we end up in a consistent state because of that assumption we had, and so on. So serial schedules guarantee consistent behavior of the database. On the other hand, it's quite inefficient because you have to wait until the transactions are done before you can start the next one. So we also have this multi-national concurrent schedules. In general, they may not execute correctly, they may not guarantee consistency of the database or isolation, but they are very efficient because they can interact with this transaction in very strange ways. Basically, if you get asked to do something, you just do it and things are fine. So, but I haven't really shown this part about it not executing correctly. I haven't shown this part about not granting consistency isolation, and I haven't actually shown this part about efficiency either. So the next part, I'm going to show all of the three things first concurrent schedules, and then I will talk about what a good trade-off between the two is. So to show that there exists a concurrent schedule that is not behaving similar to any serial schedule, I will use this example here. I'm also going to use it later again, and ensure that there are good concurrent schedules that are not serial. So what these two transactions making up a serial schedule are doing is, transaction one we're reading in X, adding N to X, writing out X, adding, uh, reading in Y, adding N to Y, writing out Y, committing in transaction one. That was everything in transaction one. Transaction two is reading in X, adding M to X, writing out X, and committing. So the example of a bad concurrent schedule that's running on the same underlying set of transactions is this one. So what it's doing is, it's first reading in X in transaction one, then it's reading in X in transaction two, adding M to X in transaction two, adding N to X in transaction one, writing out in transaction one the X variable, reading in Y in transaction one, writing out X in transaction two, committing transaction two, reading in, uh, no, sorry, adding N to Y in transaction one, and writing out Y, and then committing. And as we'll see when we go through it in detail, this will have a very different behavior from any of the two underlying serial scheduler for these two transactions. So note that we can do this kind of concurrent schedule in all kind of ways. We just need to ensure that we have that each transaction that's happening in the right order. So we need to ensure that readX comes before X plus M, comes before Y divided by X, and commit in transaction two, and similar for transaction one. Now I wanted to show you that this schedule is more efficient than any serial schedule for this pair of transactions. So let's say we have the statements coming in from whatever source they're coming in from at the time denoted by this concurrent scheduler. So time one, we get this readX in from transaction one, for instance, at time five we get a write out X in from transaction one, and so on. So at each point in time, we get the corresponding action in as in the concurrent scheduler, and we can assume that each command takes one time unit. Now if you execute a command when you get it, then you get this concurrent scheduler, and then you finish at time 11. However, if you're doing it serially, then you cannot do it before time 15. Why is this? Well, you can either try to first do transaction one and then transaction two, or then first do transaction two and then transaction one. Let's see the first one by first two, transaction one, and then transaction two. Well, we cannot start transaction two before we have committed transaction one, which happens at time 11. So at that point, we do the four operations at time in transaction two, which takes us to time 15. On the other hand, doing the second one first and then the first, which also gets us to time 15, well this is because transaction two stops at time eight, and transaction one consists of seven statements that why take seven more time units landing or at that time 15. So here we see that both serial schedules will take 15 time units, while this concurrent schedule will take only 11 time units to do this operation, and that's a bit faster than what we could do otherwise. On the last slide, we saw that this concurrent schedule would always be four time units faster than any of the two serial schedules. So does that mean that in some sense this would be a better schedule? Is it always doing the same kind of thing as the serial one? The answer is no. This concurrent schedule is doing something else than any of the two serial schedules is doing, and therefore we don't think it's a good concurrent schedule. Right now this is just a claim. Let me try to walk you through it and see precisely what this concurrent schedule is doing, and then we can compare it to the two serial schedules afterwards. So to explain precisely what this concurrent schedule is doing, I'm going to talk about these read and write operations in more detail. So I'm going to remind you first what they're doing, as it comes from this video on the schedules, and then sort of show you in details how this concurrent schedule is in one. So BDX previously still defined as that we read the database item X into a program Y named X, also for simplicity, and what we do more precisely is we find the block containing X, then we move it into the buffer if it's not already there, and then we copy it from the buffer into the program variable for variable X, when we do a read X operation. On the other hand, whenever we do a write X operation, we do it in reverse. Well, kind of. We start by still finding the disk block containing X, then we move it into the buffer if it's not already there, then we copy X from the program variable into the buffer, and then we're moving it out at a later point in time, or sometimes sometimes directly out onto the hard disk. Later on we'll be more precise about this, but right now this is enough position to see why these two things are different. So I'm going to go through this schedule, what it's doing precisely in details. So we have the setup here, we have the schedule on one side, and we have the computer basically on the right hand side. So we have two transactions, transaction one, transaction two. Transaction one has two program variables X and Y. Transaction two has only one, namely X. We have the buffer, which corresponds to RAM in the computer typically. It corresponds to other things if you have multiple servers, but let's just think of it as one computer right now, and that case is called Montmorham, and then the hard disk is put out on your hard disk. The database is out on your hard disk. Sorry. So let's see what's going on. So first we have some values for the variables on the hard disk. X has value one, Y has value two. It doesn't really matter as such for the example, but we should start with some concrete values so we can see concretely what's going on. So the first thing we do is we read in X in transaction one. So what it does is it puts it into the buffer first, because it wasn't in the buffer, and then from the buffer we move it into transaction one's program variable X. Then we read X in transaction two, and again in this case we already have it in the buffer, so we don't need to go to the hard disk. We can just take it from the buffer, put it in transaction two. Now we increase X in transaction two to buy M. So it was before one, now it's one plus M. Also in transaction one, we're increasing it to X plus N, so it's one plus N in this case, where X started as one. Then we're writing out in transaction one. That means we're putting it into the buffer. Sometimes we can also pull it out to the database. In this case we don't do it right now, we just do it at a later point in time. It doesn't really matter for what will happen here at the end. Even if we had put it down in the database and written one plus N, it would still have done the same thing. Now we're reading in Y in transaction one. That means we are reading in from a database, moving it into the buffer, because it wasn't in the buffer before, and then into transaction one. So now Y in transaction one has the value two, it's also the same in a buffer. And at this point we're writing out in transaction two. That means we take the value of X in transaction two, which was one plus M, move it into the buffer. Before it was one plus N, now it's one plus M. And then we come in, say we're moving it down at this point into the hard disk. Now the hard disk has X to be one plus M. And then we are finishing up transaction one. So we're adding N to Y, so it gets two plus this N. We write out to the buffer this two plus N, and we commit. Now that was fairly straightforward really, how to see it. I imagine many of you though are a bit surprised, sort of precisely what is happening here, as an eastern section has their own program variable, and it's quite important to keep this in mind. And they update their own site only and stuff like that. Now why is this then different from what a serial schedule will do? Well, if you look at a serial schedule, in either case, if you do transaction one first, and then transaction two, and transaction two first, and then transaction one, you will end up having X to be N plus M. But in this case it's only one plus M, so that's different, at least if N is different from zero. So yeah, as I said, it's incorrect that it should be one plus M. It should be one plus N plus M, because that's the case for both of the serial schedules on this concurrent schedule. So the question is, we're interested in how much concurrency can we allow, while satisfying isolation and consistency? As we saw, in general, concurrent schedules are more efficient than the serial schedules. But they might not do the correct thing, so we like it to be as concurrent as possible, while satisfying isolation and consistency, or allowing at least as much concurrency as possible, while satisfying isolation and consistency. So we have already answered this question earlier, when we talked about consistency in ACID. The answer is that you should do something called serializable schedules. This is the best possible answer. So what is the definition of that? Let's see. It's like this. So if you have a scheduler S that is serializable, then what it means, it is that I exist some serial scheduler S prime, that an all initial database states does the same as S. Let me show you an example of this. Let me show you that this is not just an empty definition, that there are actually schedules which are serializable, but not serial. So this would be an example of it. It's actually the same two transactions from before. transaction one was doing this x plus n, and then writing out x, then y plus n, and then writing out y, and transaction two was doing this, say, was adding m to x, and then writing it out. But here we have interleaved it not in a serial way, as in here we have first the part about x, as in the first three operations of x, that was about x from transaction one, then we have entirety of transaction two, which again was about x, and then we finish up transaction one, the part about y. And if you think about it, that does the same thing as this serial schedule, the one that starts with transaction one, and then do transaction two afterwards. This is because, well, we have just moved the part about transaction two, that has to do with x a bit earlier, and that doesn't do anything, because transaction one was at that point already finished looking at x. So that's why these two schedulers are doing the same thing, on its own input state. So now we have seen that some schedules are serializable, but not serial, and we've also seen this example of a schedule which is concurrent, but not serializable, because it doesn't do the same thing as a serial schedule. So yeah, we see that it's sort of neither all the concurrent ones, and neither only the serial ones, these serializable schedules. So serializable schedules are in essence those things we're looking for. They guarantee correctness and consistency, because it's guaranteed by serial schedules. However, they do not quite guarantee isolation, as we saw even in this example earlier. It wasn't doing even this low level of isolation where you had read committed. It was actually reading things uncommitted, but we could fix this. It wouldn't be a big deal, be it to fix that part. However, and this is a big deal, they're very difficult to test if a schedule is serializable in general. This is because it doesn't just depend on the read, write, and commit statements, but also on the non-database operations, and these are in general quite complex. As new and quite complex non-database operations, what they're doing. In a later course, you will find out that what deciding if a schedule is consistent, or is serializable or not, is what is called true and complete. What it means is basically that no computer program can do it and be guaranteed to finish ever, which is a big problem, because we want to know whether or not we have a good schedule here. So, serializable behavior is not good enough, because we cannot recognize it. We need something a bit more, a bit stronger requirements, so that we can recognize it easily. However, I'm going to from now on assume that serializability only depends on read and write operations. It's still quite difficult to solve. It's not true and complete anymore, but it's still very expensive. This serializable scheduler is precisely the answer to the question of how much concurrency we can allow while having consistency. And while we didn't have isolation, we could fix that, or at least I claim we could fix that. But they cannot be recognized. Now you might think, ah, it doesn't really matter if you cannot recognize it. But if you cannot recognize it, then you cannot construct them either, because if you can construct them always, well, you can just choose that to figure out if it's serializable or not. I didn't show that they cannot be recognized, though it will come in a later course that will show you what this true and complete means. And you will then be able to think back on this and say, yes, they cannot be recognized, because this question is clearly true and complete. So a scheduler was serializable if there was a corresponding serial scheduler that on all initial database states would do the same thing. But I really didn't answer the question I set out to do. How much concurrency can we allow while having an inner serial scheduler, while still having consistency? Because I couldn't figure out how to construct such a scheduler. So what we're going to do next is to see what is done in database management systems, how they are solving this problem, and what concurrency they are allowing in the serial scheduler while ensuring consistency. And that's going to be easily easy to recognize and I'm going to show that in two videos from now.