In this video, I will discuss timestamp-based schedules. So, as an overview, in the last video, I discussed how we could add deadlock prevention into our strict 2PL system by among other approaches using these timestamps. In this video, however, I will show you how to do an entirely different approach to schedules that ensures deadlock freeness by default, simply by using these timestamps directly to schedule things. So, as I said earlier, there are two general approaches for deadlock prevention. You can either detect them and then fix them, or you can try to enforce deadlock free schedules directly. Last video was the first one, this video is the second one. And here we are also going to use timestamps. The basic idea of these timestamp-based approaches to doing scheduling is very simple, really. The idea is just that you think of the schedule as being executed at the time it started, as an added timestamp it started at. And anything that breaks that is a paradox, and therefore you are aborting things in that case. We will see how to do it in a bit. So, basically, we have time here going right. And say T1 starts here, T2 starts here, and T3 starts here. We don't like things that start on top of each other here, so we go into view T3 as being starting slightly later than T2, just to have them at different points in time. And then later on T4 might start. So, basically, what we want to do is the serial schedule where T1 starts, then T2, then T3, then T4. Of course, if it had been first T3 and then T2, then it would have been the serial schedule with 1, 3, 2, 4. But in this case it is 1, 2, 3, 4. Intuitively, we are still trying to answer this question about schedules, where we have a scheduler that gets fed operations from a transaction manager that has some number of transactions. And then it sends requests from the transaction manager down to the scheduler. Like, this could be read or write operations. And then the scheduler needs to decide, should I grant this request, should I abort this transaction, or should I delay this transaction. Most of the time, when we talk timestamp based schedules here, we will look at grant request and abort transaction. So if something wrong happens, we just abort. It is not quite true when we add strict schedules into the mix, as in this isolation properties and stuff like that, which we will do towards the end. We will also be using delays. But most of the time, when you are talking timestamp based schedules, you are using grant request or abort transaction. Let's see how it works. So let me get back into this timestamp. It is quite close to what I talked about in the last video. So each transaction gets assigned a timestamp TS(t) when it starts. So the starts part is the only difference between this video and the last one. In the last one it was arrived instead of starts. The difference is when I say starts, I mean when you restart a transaction, you get a new timestamp. So we have time running towards the right here and say t1 starts first and we assign it a timestamp of 1. And if you start before some other transactions, then we require you to have a smaller timestamp exactly like before. And exactly like before, we say that a transaction that starts before some other transactions is older than the other transaction. And if you start later than another transaction, you are younger than that transaction. And again, I want to highlight again that you assign a new timestamp even after restart. This is not the case in the previous video. There we just got a timestamp and stuck with it. So now we have t1 first, then comes t2. And of course, there is a bigger number like 2. Again, it could also be 165 or some other number. It should just be bigger than 1. Then t3 might start and it gets a timestamp of 3. And so on. So t4 might start and get a timestamp of 4. And like in the last video, we have t2 reports here and restarts. But here, because we assign timestamps to you when you start, you get a new timestamp number. In this case, a transaction 2 and stuck with number 5. So again, I just want to reiterate this quite a few times. Notice that this is quite different from what it was in the last video, where you get a timestamp when you arrived and not when you started. So this is why you get a timestamp of 5 here for t6 and 2. So we need to do a bit more work when we talk about this timestamp-based approach to schedules, because, well, this strict 2PL was reasonably complicated. So of course, that should also be a reasonable amount of complications to get it done in some other way. So in this case, the way we do it is we have for each database item, we maintain some numbers. Specifically, we have something called read time of the variable. So read time of variable x, for instance, is a timestamp of the youngest transactions that last with x. We also have a write time, which is basically the same thing, except you, instead of the youngest transaction that read it, it's the youngest transaction that wrote to it. So let me come up with an example here. Say the read time is initially 0 for x, and the write time is initially 0 for x. It should just be smaller than the smallest number you use for your transaction. In this case, we use positive numbers for the transaction's time, so that's why we use 0 here. Then, say we have a transaction 2 here, it reads x. And then, at that point, because the youngest transaction that I ever read x has been this transaction, we get the read time of x to be 2. Now, transaction 1 might try to read x. It has number 1, and the read time is still going to be 2. Even though that transaction 1 has now written also, it's not going to change, because, well, 2 is bigger than 1. So basically, yeah, the read and the write times are the max of the numbers that has ever loaded this item. Later on, we might have that the tree is reading it also with timestamp 3, and in that case, we update the read number to number 3, because now the max has been 3 for this item. And some data points, we might have transaction 2 writes to x, and in that case, we update the write time of x to 2. And this is just a procedure, how you do it, to update this read and write times. And finally, we might have that transaction 4 writes, and again, it works similar to read time, so you just update the number to the maximum, and in this case, the maximum would be 4, because, well, transaction 4 wrote to this item. Notice that read time of x is not influenced by the fact that we have written to it at timestamp 4. So, these timestamps are all we really need to figure out if you can grant requests or not. So let's first look at read requests, and then we will look at write requests next. So basically speaking, if transaction T1 requests to read x, we abort and restart transaction 1 if the write time of x is bigger than the timestamp of transaction 1. So let me try to illustrate it. We have transaction 1 here. It has this timestamp here. And as I said, following the basic idea, it was executed at this time. Now we have the write time of x over here. It might be written to by transaction 2. So this is T2's time that was executed, if you follow the basic idea where you just execute at your timestamp number. And of course, here we cannot have the T1 read something written by T2, because T2 has not done yet, so the update shouldn't have happened. And because it shouldn't have happened, well, this would be a paradox, and therefore we restart transaction 1. And if we don't have this issue that the item was written in the future, we just grant the request, things work okay right now. So let's look at the write request next. Intuitively, it's the same principle. So we're trying to avoid all the paradoxes, and let's see how we handle that. So transaction 1 requests the write to an item x. In that case, we are bored in two cases. We are bored if the read time of x is already passed transaction 1, or we are bored if the write time of x is already passed the timestamp of transaction 1. Why do we do it? Well, let me try to explain the read time first. We do it for the read time, because whatever read from x was supposed to read whatever was written by transaction 1, instead of whatever it was really reading. And we also do it in the other case with the write time of x, because otherwise transaction 1 would be overwriting something that should happen in the future first. So it might look like this. We have time running here to the right, and if T1 was executed here, and we have the read and write time over here for x, which was T2's time, then this, if we have this situation, then the request for T1 to write x, T2 could not have read whatever was written by x at this hypothetical timestamp, and it could not have overwritten the correct value of x. So this is basically the intuitive reason for why we have precisely this requirement. And if we don't have this problem, we just grant the request and move forward. There's no paradox in that case, and we can just move on. So let me try to illustrate it using an example here. We have two transactions, T1 and T2. Initially, transaction 1 has timestamp 1. Initially, transaction 2 has timestamp 2. And we have two variables in our database, x and y, and they have the read and write numbers over in these two columns to the right. And initially, those are zero, because as I said, they should be smaller than the smallest number of transactions. Since we start with 1, they can be zero just fine. So we then read from x here. We update the read time of x to 1, because that's how it done. So x is last read at time 1. Now we add 100 to x. It doesn't change the read or write time for anything. Now we might read y in the other transaction. So here the read time of y is up to 2, because y was last read at time 2. And then we might multiply y by 2. And write out y. When we write out y, we observe that y has last been written at time 2. So, yeah, now we're in this situation. And then x might, transaction 1 might try to read y. However, if it does it, it will end up reading this value written by transaction 2. And therefore we must, therefore we have a paradox and must read a port and restart transaction 1. So that's sort of the idea. If you run into a paradox where you see data bits are too new or try to overwrite data that are too new, we must restart the transaction and start over. So, yeah, when we abort, we also remove the read time of x back to 0. Because now transaction 1 has not read it, because, well, it hasn't done anything when we do an abort. So now we get a new timestamp for transaction 1. Let's say we give it number 3. So here, now we start over. So we start over by first read x. So that now the read number for transaction 1 was 3. And therefore we update read time of x to 3. And we add 100 to x. This doesn't change the read or write time for anything like that. We read y. That updates the read time of y to 3. And we are good to go, basically. And we multiply y by 3. Again, this doesn't change the read or write times. And we write out y. So that updates the write time of y to 3 also. And this is the principle of how this timestamp-based process works. Let me try to illustrate with a second example an issue we also had earlier, namely cascading rollbacks. So let's look at this transaction 1 and this transaction 2. Here, transaction 1 is first reading and writing to x, and then reading and writing to y. And transaction 2 is doing the opposite, as in first reading and writing to y, and then reading and writing to x. And say we have one of these timestamp-based schedulers, and t1 starts first. And we first do lines 1 to 3 of t1, and then lines 1 to 3 of t2. And what can we do next? So here we start at timestamp 1 for t1, and timestamp 2 for t2. And we read first the tree lines. We do first the tree lines of transaction 1, as we say. Then the first three lines of transaction 2. We are down here. And one of the options we can do is continue on in transaction 2. So now it has read and written everything it needed to do, and is somewhat done. But what happens if we move forward over in transaction 1? Well, we have to restart transaction 1, because the read time of y is 2, and transaction 1 has only timestamp 1. Therefore, we need to restart. And that will roll it back. And when we roll it back, well, it has already read and written something to x, and this was read by transaction 2. So we will get a cascading rollback. Because, well, yeah, as I said, x has already been read and written by transaction 2. So let's try to do the same start again. First transaction 1, do the first three lines, then transaction 2, just the first two lines, sorry. And then we continue over in transaction 1. And we get to here, this read y, well, it's going to crash, because the write time of y is already 2. Because it was written by transaction 2 just now. So, again, we get a rollback, and we change this down to 3. And then maybe transaction 1 again does the first three lines, and then transaction 2 might try to proceed. Well, then it gets down to here to read x. But x has just been written by transaction 1, who now has timestamp 3. So that's a paradox. We need to restart transaction 2. And we give a new number to transaction 2, say 4. Then we might do it the same again. We might have transaction 2 do the first three lines. And then we can proceed over in transaction 1 again. We get down here. We have to rollback. And as you can see, we can just keep on going over and over with this pattern here. We keep on rolling things back and back and back. And this problem is called starvation, and it's also a problem. Well, it's a well-known problem for these timestamp-based approaches. So instead of having deadlocks, they have starvation instead, basically. Well, they end up having to get stuck somewhere and just keep on restarting and restarting and restarting without ever finishing, in some cases, if you're unlucky with your execution. So these kind of timestamp-based approaches to scheduling has some nice properties. They automatically ensure conflict serializability, because, well, they act as if everything was executed when they started. And that will, of course, give you conflict serializable schedules. They also ensure that deadlocks cannot occur entirely by design. So that's also a nice property, because, well, we saw in the last video that you need to spend some effort to remove these deadlocks. Otherwise, and that's nice not to have to do that. On the bad side, they still have these cascading rollbacks, and they also have the starvation thingy, where you can get cyclic reports and have to restart transactions. I don't want to cover the starvation part in this module, but you can do it basically using some techniques for it. Again, I don't want to cover it, but yeah, it is mainly, in my point of view, a parallel to deadlocks, but for timestamp-based approaches for scheduling. I will show, however, how to deal with these cascading rollbacks. The idea is basically the same as we used before. We ensure our schedules are strict. So how do we ensure that our timestamp-based schedules are strict? Well, it doesn't happen by default, but what you do to ensure a strict schedule is to delay, read the write request until the youngest transaction, borrowed X before, has committed or bought it. So you basically just delay things now. We haven't used delay before, but as I said, it will come here towards the end to, in this case, deal with cascading rollbacks. So say we have our request to read or write item X here at timestamp 12, which is the timestamp of transaction 1. We might have that X was written to a timestamp 2, say by transaction 2, and also at timestamp 5 by transaction 3. This means that we have to delay this read or write request for X until the time at which transaction 3 has committed or bought it. At that point, we can proceed with reading or writing X, and that's fine. Very many database management systems implement these time-based approaches for scheduling, but they implement a variant of it as compared to what we have seen before. This variant is called multi-version concurrent control. So the idea is exactly as in timestamp-based approaches to scheduling. However, you have multiple versions of your database. More precisely, whenever you do a write operation, instead of overwriting things in the database, you create a new version of the item and give it your timestamp. So for instance, if you are timestamp 3 and you write to X, you have a version of X at timestamp 3, and that's the one you write. And whenever you read something, you read the latest version that is before your timestamp. So if you have an item at time 0 and at time 2, and you read at time 1 this item, you read the item as it was at time 0. And specifically not the one it was at time 2. We'll see some examples, so hopefully it becomes clear. So if you think about what it needs to do here, we only need to restart in one case compared to the previous three cases. So the one case where we need to restart a return section is if you try to do a write at the same time as the read timestamp is later than your timestamp. So let me try to do it formally. The rules are as follows. For writes, you are bored and restart if the read timestamp is later than your timestamp, and otherwise you grant a request. If you do a read operation, you always just grant the request. So this is different from before. Before we had that read timestamps and write timestamps should be beyond your own timestamps, or one of them should be beyond your own timestamp. In that case, we are bored and restarted. Now we just need that the read timestamp is beyond our timestamp for our abort or restart. And before we also checked if the write timestamp was beyond our timestamp when we did a read operation, now we can omit this because we are just reading things in the past in a way. So basically the rules are somewhat simpler. There's at least fewer of them, and therefore it allows us to do more operations faster. However, of course, it caused us in that we have to store multiple versions of each item. There's also a strict way end of this where you delay reads until the transaction you read from has committed. And again, that will ensure no cascading rollbacks. Otherwise you will have cascading rollbacks in this stuff. So let me go to an example for it, this example one from before, to see how it works. So example one from before, it was this one. We had two transacts in transaction one and another transaction two. Transaction one started at time one, time section two started with timestamp two, and we have two variables, x and y. We don't really formally need the write timestamps anymore, but I'm going to include them just to have the similarity with the previous version. So at timestamp zero, we don't have any requests to the timestamp, so they're all just zero. And we also need to talk about what version of the item you see now. So I'm going to say that timestamp zero, the variable x has value 10 and the variable y has value 10 also. And then we do a read operation at timestamp one in operation one. And that updates the read time to one from zero. And it's going to read this version of x. Now we add 100 to x. It doesn't change what is in the database or anything we haven't done in the writes. And then transaction two comes along. It reads y. This updates the read timestamp of y to two. And it's going to read this version of y. Now transaction two is multiplying y by two. That doesn't change the read and write times, because there is no read or write. And then it's going to write y out in the database. So it has updated the write time of y to two. And it looks like this. So now in timestamp two, we know that the value of y is 20. Notice that we don't have a value for x here, because transaction two has not written any value for x. So now we get this read y operation from before. Now earlier it request in the basic version of timestamp based approaches. However, now it succeeds because it's a read request and we can just read things backward in time. So we don't change the read time here because it's already two. And the version of y it's going to read is this one up here. It's not going to read the newest one, but the old one. And then we multiply y by three. And that doesn't change anything in our read or write times. And now it tries to write out y. However, this is going to cause an abort and restart, because the read timestamp of y is two and the timestamp for transaction one is one. So we get a new timestamp for transaction one, namely number three, and add a new row into our table. So two corresponds to our timestamp three. And we insert a new row into our read and write times operator to zero for x, because now we have a border transaction one and therefore x has not been read yet. And also if we had written anything at timestamp one, which we didn't get, it would also have been deleted from the update when we aborted transaction one. So now we have aborted transaction one and this is the situation we are in. And we start again. So now we do the read operation first on x. And it's going to update the read time to three on x. And it's going to read this version of x, because transaction number two never wrote anything to x. And then we add 100 to x. It's now 110. This doesn't change the read or write times. And now we're going to read y again. And like before, it doesn't, because however, this time it's updating the read timestamp to y to three. And it's going to read the version of y that was written out by transaction two, because three is bigger than two. And now we're multiplying y by three. And that doesn't change the timestamps. And then let's do the last operation, namely the write y operation. And that's going to update the write time of item y to number three, because, well, it's timestamp three for transaction one now. And it's going to create a new row, a timestamp three, and in this new row, the value of y is 60. So that's how this multi-version concurrency control works on this example. I hope it's reasonably clear what's going on. Let me summarize. We have seen that we can do some alternate techniques instead of doing this strict Jupyel to ensure all the ACID properties, and we avoid deadlocks automatically. The idea is just that you act as if all your transactions did everything they needed to do at the time they started. And this might cause some paradoxes, and we can deal with them by restarting transactions that causes paradoxes. So this only gives you conflicts for eligibility, but you can ensure strict, as in no cascading rollbacks, by on each read or write request that will not create a paradox, wait until the transaction that last wrote to that item has committed or aborted. And we can do this idea in two ways. We can do the normal version, where you just have a single version of the database of each variable, as we saw in the beginning of this talk. Or we can do it in this multi-version concurrency control variant, where each write operation is saved with its own timestamp, so you can read things at any point in time.