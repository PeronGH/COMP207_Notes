In this video, I will talk more about this Market Basket Model. It's a bit generic title, but we'll see what I mean. If you're watching these movies in order, this is the last proper video about the content of the course. There will be one more video, which will be exclusively about the kind of questions you will see in the exam, which is basically the kind of question you saw in the second assignment. But here, I will talk a bit more about this Market Basket Model from the last video. I will talk a bit more about complex tables, then association rules, and then this algorithm I squashed in the first video, called the Eprioy algorithm, as in the one where you're sort of finding the support of bigger and bigger sets. But first, I'll talk a bit more about more complex tables. So, sometimes we have more information than what we looked at in the last video. So, sometimes you might have a column about who bought the items, as in, say, you have extracted this piece of information from credit card used, or club card used, or video cameras, or something like that. You can figure out who bought these items. So, for instance, here, it's basically the same situation as last time, except I have added in one more column about who bought it. And in this case, the first and third transaction was done by the same person, while the other two transactions was done by two other people. And, well, now we need to ask ourselves, who are we actually talking about, when we are asking this kind of questions. So, items are clear. It's still going to be the same items, but what are the baskets in this new set of... Well, we can consider basket with respect to a column. There will be some exercises about it in the next tutorial, but let us also see the example here. So, how do we do it? Well, let's try to look at the basket with respect to customer ID. So, we have one basket per customer ID, and the basket for the customer ID CID is all items that appear under item support in rows that have customer ID CID. And it's the union of all of these rows, if you're talking about it in math. So, for instance, the basket for A would be, well, milk, bread, cookies and juice from the first row, and then also eggs from the third row. So, notice that we don't have quantities here, so we don't say that we have milk and bread twice. We just have one of each item here. We just take in the disjoint union of the two. So, another way we can do it is just to restrict it. So, we remove this column here and then combine the IDs for the same customer IDs, and we get this table here. And now we're basically just asking the same question as last time on this smaller table here. Doing that, we get this simpler set of baskets. So, we have now just customer ID and the items report. And we can then define all our notions using this new table. Specifically, we can define the support of set J with respect to the customers, its support of J computed using the new set of baskets. It's very straightforward. We can also talk about whether an item is frequent with respect to customers. If it's frequent, if it's support with respect to customers, it's at least the support to us all. This is per meter from last time. As I said, also last time, it depends on the application, what you had in mind precisely, so it cannot give you a general rule of what that number should be. Let's try to do a simple calculation on this table here. So, say we have milk and bread. Well, with respect to the postcards, we see that in the full table, it's in line one and it's in line three. So, that's two lines out of four. That gives you half. If you look at this smaller table with respect to customers, we will see that only A has both both milk and bread. So, it's only one out of three customers are supported or 0.33. So, it's support to us all by half. Milk and bread is frequent with respect to the postcards and it's not frequent with respect to customers. So, yeah, you have to be a bit precise about this kind of thing. So, let me try to do another example here. I'm not going to specify exactly what items I'm calling here. I'm just going to use letters. So, again, it's the same set of before. I just changed the set of items. So, what is the support of the double V comma X here with respect to transactions first? Well, it's going to be one out of four because only line two has both double V and X. The other three lines does not have both double V and X. They have at most one of the items. So, what is the support for double V comma X with respect to customers? Well, it's going to be two out of three. This is because A has in the first time, he bought something, he had bought X and the second time he bought double V. So, together he has bought X and double V. And the other two, B has still bought double V and X and C has not bought double V and X even with respect to customers. So, part of the reason for showing you this example was, besides showing you a second example, it was to show you that the support with respect to a column like customers here, it can be either bigger or smaller than the support with respect to the full table. So, here the support with respect to customers is bigger than the support with respect to the full table of the transactions. And if you go back to the earlier example, you will see that it was the other way there. There you had a smaller probability for the customers than for the transactions. So, in general, there's not really a strong connection between the two, except that if the support with respect to transactions is positive, then it's also going to be positive with respect to customers. Let's move on to some other concept here, which is called association rules. So, it's about why end of this frequent items at mining query. Before, we just asked, is this set of items frequent or not? Here, we are talking about a subset of our full database and asking whether they frequently bought something else. So, for instance, we might find that customers who buy diapers frequently or also buy beer. As I mentioned earlier, this actually is the case. And people watching Harry Potter and Game of Thrones frequently also bought Twin Peaks or similar information like this. So, in general, this kind of question looks like this. You have a set of items on the left hand side, and then you have this implies error and then a single item on the right hand side. Or I guess you can also have a set of items on the right hand side. I'm only going to focus on the case where you have a single item on the right hand side, too. So, yeah, all of these things are just items, both the things on the left hand side and on the right hand side. For instance, we might have this diapers implies beer or Harry Potter and Game of Thrones implies Twin Peaks. So, what are we aiming for here? Well, we want the support of this, of the full set of items should be high. That's not much point in it. Otherwise, as a full set of items, I mean, both the ones appearing on the left hand side together, but the ones appearing on the right hand side. And we want what is called a confidence, as in this percentage of the basket that contains the left hand side also contains the right hand side, it should also be high. Say we might say that 67% of all customers report milk, also report juice, or similar information to that. And it should differ significantly from the fraction of basket contained in J. So, you should have sort of, this rule should tell you a lot about J together with these items. It doesn't really tell you much if it's pretty much the same as the fraction of basket contained in J, because it just means it's close to independent, whether you buy this item or not, say if it's buying items. So, let's try to look at a small example here. Let's look at this milk and juice example. So, what's the point of it? Well, the support of milk and juice, when milk and juice got together in the first line and in the second line, so that's two out of the four lines we have, that's half the time. So, the rule applies in half of all cases. Also, the confidence is fairly good, the support of milk and juice. Well, milk and juice, as we just saw, according to half the lines, then we should divide by the support of milk. Milk is going to occur in three out of the four lines, so three divided by four, and in total we're getting then two divided by three, or 0.67, roughly. So, 67% of all cost animals for bought milk also bought juice. It differs at least an amount from the number of people who just bought juice. So, if you're talking about the people who bought milk, then they are more likely to also buy juice than the general set of people in the store, at least according to this. Okay, let's do a bit bigger example here. First, we are looking at the confidence for the association rule, BS, implying HP1. Well, first of all, what is the point of that association rule? Well, we see that BS and HP1, of course, together in line four, five, seven, and eight. So, it has support four out of eight. Similarly, BS alone has support five out of eight. It also occurs in line one, besides this four, five, seven, and eight. So, the answer to the first question on the slide is four out of five. Now, let's look at a bit more involved one, where we want to know, are there movies X and Y, such as X implies Y, with confidence two out of three, and support at least a half? Well, we had part of that already. We know at least which subsets have support, at least a half. We saw it from the earlier video. It was these four subsets, as in B I B S, B I H P 1, B S H P 1, H P 1, and H P 2. So, that leads us to eight subsets, and we should check each of them in turn. So, let's start with B I, implying B S. Well, it's first, we check how many do we have of B I and B S, of course, together. Well, that's line one, line four, line five, and line eight. So, that's four out of eight. So, how many times did B 1 occur? Well, it does line one, three, four, five, six, and eight. So, six out of eight, and the other one was four out of eight. So, that's not good enough. That's only two-thirds, and we wanted one with strictly more than two-thirds confidence. So, B I does not imply B S. What about the other way? Well, we still saw that the support for B I comma B S was four out of eight, or half. So, how many times do we see B S? We have already seen it earlier. We see it five times. So, it's going to be four out of five. So, that association rule does satisfy having confidence over two out of three. So, in general, if you go through each of them, we get these three association rules. We have already seen that the association rule B S in place A S P 1 has confidence four out of five, which is bigger than two out of three. And B S comma A S P 1 is also on this list. So, it also has big enough support. So, we just need to go to the other ones. There are five left. So, we should look at A S P 1 implying B I, or the other way. But if you look at that one, we see that, well, B I, we have already seen it, of course, six times. And A S P 1, of course, together with B I in line three, four, five, and eight. So, that's four lines. So, that's four out of eight again. That's not good enough when you divide by six out of eight, because it's still only going to be precisely two out of three. So, that association rule is not good enough. Let's try the other way, where A S P 1 implies B I. That association rule is not good enough either, because A S P 1 occurs in line one. So, like line two, three, four, five, seven, and eight. So, also six out of eight times. And again, when you take four out of eight divided by six out of eight, we are only going to get two out of three. And that was not high enough for our support. So, we don't have any rule about A S P 1 and B I at all. We have already seen one rule for B S implying A S P 1, what about the other way? Again, the number of times B S and A S P 1 occur together was only four out of eight. But A S P 1 occurs six times alone. So, the other way is not good enough. So, we are done with that one also. Now, we just need to look at A S P 1 and A S P 2. A S P 2 occurs only in line two, four, seven, and eight. So, only four out of eight times. That means that the rule from A S P 1 to A S P 2 cannot be good enough. We'll get the same issues earlier. It's at most four out of eight times. Or four out of six times, which is precisely two-thirds of not good. However, the other way is A S P 2 implying A S P 1. Well, it happens each time. Someone has seen A S P 2, they have also seen A S P 1. So, it's all the time. So, that's why we get one down. And you can do similarly for any other rules. You can get the precise percentages. So, let's see how to find this association rule. So, I'm going to focus on the one with high support, because it makes it much easier to do it. So, you first find all the set of all items J, which support at least S. And from there, it's easy to find the association rule to just pick one item out of each set. And do each item in turn. And look at the association rule of the remaining items, implying this last item. And if this is a good enough threshold, well, you just return it, otherwise you do not. So, the real hard part here is to find the frequent item sets. And as I said, we have this algorithm for it, I already discussed, and I'm going to show it more formally next. So, the algorithm is called a priori algorithm, and the goal is to compute all items J, which support at least S. And this was what I was choosing earlier to illustrate how to find a pair of some movies that had high support. Again, it can also be used for association rules, as we saw on the last slide. So, the key idea is that if J has large support, then all subset of J has also large support. So, you just compute the frequent item set from the smaller ones, and you start by having the frequent item set of size one, as I mentioned. So, it's the item set that contains one item that has support bigger than this. You just try all possibilities, and so it takes a near time. Now, what about F2? Well, you look at pairs of item sets that at least have a pair belonging to F1. And you take the ones that have support bigger than S among these. So, it's also fairly straightforward. Now, F3, you can take two pairs. You need to pick sort of three sets out of the F2, so that it covers all three subset of this bigger set. So, for item set I, J, K to have support S, you must also have that I, J has support S, I, K has support S, and J, K has support S. This requirement that all three of the subset has big support is very restricting, and why this works reasonably fast in practice. And you just continue like this forever, or until you reach, however, a large subset you care about. Let me do it formally. So, we have a set of items I, we have a set of baskets B, which is one of those market basket data models. We have a max size Q on the set size we're caring about, and we have a support threshold S. We have this parameter Q, because in general the algorithm has quite poor dependency on the basket's size of the output. So, we're going to output all the subsets J of I, with size of J being smaller than Q, and support at least S. So, the algorithm once this follows, we are defining two sets for each size, size one, size two, up to size Q. The first set C_I is going to be all possible set based on F, K minus one, and F_K is then going to be the subset of C_I that was actually having this right support. So, the way we're doing it is as follows. C_I is just each possible item in our set of items I, because, well, every UI we don't know anything about what items might or might not have good enough support. And then we do the following algorithm for each K in turn. First, we're checking if a subset of C_K has big enough support and is currently the best. And if we are now done, as in, if we have gotten to K equal to Q, we're going to stop, and the result is then going to be F_1, F_2, up to F_Q, as in the union of those sets. That's the subset that has support at least S and size at least Q, at most Q, sorry. If we're not done yet, we're also defining C_K plus one based on F_K. The way we are doing it is we are looking at all subset of size J, such that J has size K plus one, and all the subset of J of size K, of course, in F_K. Yeah, this is precisely the same algorithm as on the previous slide and precisely the algorithm I sketched when I showed how to use it earlier. And we can actually do this efficiently in SQL for fixed parameters Q. For instance, we can do it for Q equal two or three or whatever. But in general, as I said, it has, this algorithm has quite poor dependency on Q, and it will take exponential time and very many queries to do it in general. So in summary, here we saw some more complex tables, specifically with respect to a column. Instead of just doing it according to the full table, we saw association rules. That is, does the person of some subset of items imply that some more items are also like they are? As well as this a priori algorithm that allows you to answer all of these questions. He was also skit in the first video on this market basket data. Thank you.