In this video, I will talk about key value stores. So, key value stores are the simplest kind of database systems, basically imaginable. We'll see here what it's all about in this video. A key value store is basically a collection of table, and each table consists of only a key and a value pair. So, you have a key column, you have a value column, and the only two things you can do is, you can given a key, you can find the corresponding value, and given a key and a value, you can insert it into your database. And this is it. There's nothing else you can do in a key value store. And this can be done very fast, these two operations, because you would have a index on the key, and there's no further indices, and there are no kinds of transactions. There are basically just this. So, we just search in for our key and output it or insert, depending on what kind of query we are doing right now. And this is it. There's nothing else you can do here. In key value stores. So, in essence, it's just one big index we're talking about here. So, there are a whole bunch of implementations of it, because, well, on one hand, it's very simple, and it can still be used in a reasonable number of applications. Like, if you have, want to find a user profile based on some kind of ID for the user, well, this is what you just need. You don't need anything else you just need. Given this key, you need to find the value, the information you store about this one user. So, it's a reasonable, often used thing. So, that's a bunch of implementations of it. Especially, I want to highlight Amazon Studio Namely, an Apache folder mode, because I want to use them to illustrate some of the techniques and say where they're coming from and so on. So, the basic of this system is distributed storage. So, what you are doing is you are storing each key value pair at some nodes. The nodes here are the databases in your distributed storage. So, if you have five computers, you will store this key value pairs spread over these five computers in some fashion. The way it's done is more precisely as follows. So, step one, you assign the values for a key to an integer between 0 and 2 to the n minus 1, where n is some sufficiently large number, so that you can space out your nodes where you are storing things nicely on this ring here. And to do that, you use some kind of has function. And if you don't know what a has function is, just think of it as a random function, in essence, except that on a fixed input, you'll always get the same output. So, if you have the same key as input, you will always get the same number in this interval as output, but otherwise it's basically random. At least this is intuition of what a good has function will be doing. I don't want to go into details about has function. It doesn't really fit that well into this course. So, next, after you have figured out what has function you're using, you should distribute your nodes as in the computers among these integers. As in, you might have one computer at integer 7, 165,000, and another one at some other numbers. And you just do it like that. So, you can have multiple computers and multiple versions of the same node on many places. This is because, say, your computers are not equally powerful. Here, for instance, we have three copies of A, we have two copies of C, and three copies of B. That could mean that the computer storing C is slightly less powerful than the other two, as in it's two-thirds as powerful as A or B. And if that's the case, then this would be a fairly reasonable setup for it. So, then you just store them like this as a peak each computer for each number, and then the key value pair is assigned to the integer, or to the node at an integer that comes next. So, for instance, things between the top right A and B and A is stored at A, things between the next A and next B are stored at the next B, and so on. So, you're always sort of just storing it a bit further along the cycle. Also, this wraps around, so if you store it, say, at something at 2 to the N minus 1, then it will typically be stored on the first computer on the other side of 0. It's fairly straightforward. So, let's first look at how to get scalability in this model. Well, it's fairly easy if you have a need for more power. You just add in a new computer, add some random location in your cycle, and move the key value pairs around appropriately, so that it works. Let me come up with an example here. So, we have this cycle from before. We haven't added in C yet, and now, say, we figured out this C computer was a bit less powerful. We add in only two of them. So, we add them in, say, here and here, just by picking random numbers, and then we move some of the key value pairs from the following A back to C, and we move some of the key value pairs from the following A back to C. In this case, it was C in both cases, but in general, of course, it is not necessarily A we are moving it from. It's always just the one that comes next when you sort of run around in the cycle. And this gives you an automatic version of this horizontal fragmentation. We have split out the rows in our table out over our multiple computers, similarly to what we saw in distributed databases. Next, let's talk about how to get this availability. The way we do that is using replication as an... we don't store all information in just one node. We store them in multiple nodes. So, what we do is we just store the replicas as copies of our key value pairs on consecutive nodes in the ring in clockwise order. So, for instance, if we have this set up from before, and we have a key value here, that will normally be stored just at A, and the following A here. But, say, if we have replicated it twice, it will also be stored in the next node, which will be B. So, that means that if A failed, well, you can get it from B also, this key value pair. And if we have a replicate tin of level 3, it will also store it down here in C. So, it's fairly straightforward how to do this. So, we have already seen how to get most of the properties. Scalability, well, it was easy. You just add new nodes in by putting them in at random on the cycle and moving things around as probably. This basically gives you this automatic horizontal fragmentation also, which is kind of nice way of doing it. Availability, fault tolerance, we just saw it. We get that using replication. So, we can achieve a key, a value for key, even if a few of the nodes storing that value for that key had failed, because we'll keep on storing it in a few more in clockwise order around the cycle. We can also do this kind of queries very, very fast, because the only thing you need to do is to take this hash function, which I haven't really gone into what how you do, but they're not that slow. And then you use that to figure out which node to ask. That will give you the key number on this cycle, and then you use that number on the cycle to determine a node, and then you ask that node, and that's everything you need to do. And then you get the information back. And you can also write fast, well, you just do the same kind of thing, but of course you need to replicate it a few times, depending on the requirements for replication. Now, as mentioned earlier, we cannot quite do consistency also as a consistency here in a NoSQL database, which is not the same thing as in AZT. We cannot do it because of this CAP theorem I mentioned in introduction to NoSQL. So, what about consistency? Well, GenomeDB and Voldemort does it by allowing multiple versions of data items to be present at the same time called versioning. So, the idea is, if a newer version of a data item is not yet available, say you haven't stored it yet quite yet on all of these replicas, you just get the newest version which is available. And this is typically fine for many applications like shopping cart. If you're missing the last few items, it hasn't quite made it yet, maybe you will just get it sooner. It is typically fine most of the time that it is not really there yet. So, the way you do it is you assign a vector clock to each version of an item X, which is a list of node and timestamps as in of each of the replicas that should store X. So, the node corresponds to where it stored and then we have a timestamp which is a local time on a node right where this item was written. So, for instance, it could look like this. A1, B5 and C3. That could be a vector clock for an item X if the last time X was written to was at time 1 and A, time 5 and B and time 3 and C. And then we use this clock to decide if a version v1 originated from another version v2. And the way you do it is you just check that each number is smaller than or equal to the other number in the other vector clock. So, for instance, this would be an example value smaller than or equal to because each number is smaller than the right hand side and in this case one of them was also bigger. So, that's again it's fine. The number should just be bigger than or equal to the previous ones. Of course, you can also have things that are incomparable where some number are bigger and some number are smaller. In that case, you just take in all these options and return all of them and let the user deal with it. That's how it's done in NaomiDB and Voldemort. They just would give you all of the ones that are not really comparable to each other. Formally, they call it conflict, but I didn't want to introduce that also because that would just make it more and more annoying to follow it. So, I'm calling it incomparable in this course even though they will call it a conflict. And this incomparability are then resolved at read time. So, in summary, we have seen key value stores here. There are some typical techniques used for them. For instance, we have this distributed test tables as in having this cycle of values where we store some of the nodes on it and they store key value pairs. We have seen this replication where you just store the item not just at one node but also at the following nodes so that if one goes down, you can just ask the next one in line. And then we have also seen this versioning and incomparability resolution using vector clock session. You just have a version that tells you about the timestamps of the different nodes by the store and return them, all of the ones, all the versions of it that are incomparable to each other. And the way you do it is that we just return local data and write first update local data which will then be propagated on to the other sites that replicates the data.